---
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br>
<br>

## GWAS/popgen crash course

<br>

### Today’s data

<br>

We are working with WGS data from the 1000 Genomes Project (1kGP). The final, phase 3 release of the project included from 26 populations across five continental regions of the world and was based primarily on low-coverage WGS. It culminated in 2015 with publication consisting of 2,504 unrelated samples ([The 1000 Genomes Project Consortum, 2015](https://www.nature.com/articles/nature15393)). Today, we will use the high-coverage WGS of the original 2,504 1kGP samples, as well as of 698 additional related samples that now complete 602 trios in the 1kGP cohort ([Byrska-Bishop et al., 2022](https://www.sciencedirect.com/science/article/pii/S0092867422009916?via%3Dihub)). We could check the sample metadata [here](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000G_2504_high_coverage/20130606_g1k_3202_samples_ped_population.txt).

<br>

For a bit more fun and continuity between the GWAS workshop and this part of workshop, we also merged the 726 Tomsk samples with these 3,202 samples from 1kGP. Therefore, there are 3,928 samples in today's dataset. For speed and simplicity here, we will perform all of our analyses on only a single chromosome - **chr1**.

<br>


In this session, you will learn how to:



*  [Perform a PCA]
   * *[Optional Step 1: Linkage pruning]*
   * *[Optional Step 2: Perform a PCA]*
   * [Step 2: Perform a PCA]
   * [Step 3: Plotting the PCA output]
   
<br>

*  [Compute per site *F*<sub>ST</sub>]
   * *[Optional Step 1: Calculating the Mean genome-wide *F*<sub>ST</sub>]*
   * [Step 2: Check the Mean genome-wide *F*<sub>ST</sub>]
   * [Step 3: Visualising per SNP *F*<sub>ST</sub>]
   * [Step 4: Performing a simple outlier analysis]
   * [*Bonus Step 4: Performing simple outlier analyses using a loop*]
   
<br>

*  [Estimate the site frequency spectrum]
   * *[Optional Step 1: calculate the allele frequency]*
   * [Step 2: plot the SFS]
   
<br>


> ##### Please download the dataset [here](https://collect.qmul.ac.uk/down?t=554IPEA99A1H5MHG/6L4DLI5BDRJOQHT4GBI8MB8).

For those who want to perform the optional steps, please download the raw dataset [here]() (**very large**, ~3GB for the zipped folder).

<br>

--------------------------------------------------

<br>

## Perform a PCA

<br>


First of all, we will investigate population structure using [principal components analysis](https://en.wikipedia.org/wiki/Principal_component_analysis). Examining population structure can give us a great deal of insight into the history and origin of populations. Model-free methods for examining population structure and ancestry, such as principal components analysis are extremely popular in population genomic research. This is because it is typically simple to apply and relatively easy to interpret. 

Essentially, PCA aims to identify the main axes of variation in a dataset with each axis being independent of the next (i.e. there should be no correlation between them). The first component summarizes the major axis variation and the second the next largest and so on, until cumulatively all the available variation is explained. In the context of genetic data, PCA summarizes the major axes of variation in allele frequencies and then produces the coordinates of individuals along these axes. 

To perform a PCA on our human data, we will use `plink` - specifically [version 1.9](https://www.cog-genomics.org/plink/1.9/) (although be aware [older](http://zzz.bwh.harvard.edu/plink/) and [newer](https://www.cog-genomics.org/plink/2.0/) versions are available).

`plink` is pre-installed on Apocrita as a [module](https://docs.hpc.qmul.ac.uk/apps/bio/plink/). There are several versions.

```{bash,  eval = FALSE}
# check the versions of plink installed on Apocrita
module avail | grep plink
# we will use plink/1.9-170906 
ml plink/1.9-170906 
# check the loaded version 
plink --version

```


<br>
<br>

#### Optional Step 1: Linkage pruning
<br>

One of the major assumptions of PCA is that the data we use is indpendent - i.e. there are no spurious correlations among the measured variables. This is obviously not the case for most genomic data as allele frequencies are correlated due to physical linkage and linkage disequilibrium. So as a first step, we need to prune our dataset of variants that are in linkage.


First things first, we will make a directory called `ld_pruning`.

```{bash,  eval = FALSE}
# make a directory
mkdir -p ld_pruning
# move into it
cd ld_pruning
```
<br>

We will build our job script for the linkage pruning and use `qsub` to submit your job (you could also run it on your own laptop or use [`qlogin`](https://docs.hpc.qmul.ac.uk/using/#interactive-jobs) to run this interactively). We will breakdown what all the arguments mean later.
```{bash,  eval = FALSE}
#!/bin/bash
#$ -cwd
#$ -j y
#$ -pe smp 1
#$ -l h_rt=1:0:0
#$ -l h_vmem=1G

ml plink/1.9-170906
VCF=../merge_data/g1k_TOMSK_lifted_GRCh38_merged_chr1.vcf.gz #insert your data path
OUT=g1k_TOMSK_lifted_GRCh38_merged_chr1

plink --threads ${NSLOTS} --vcf $VCF --double-id \
--set-missing-var-ids @:# \
--indep-pairwise 50 10 0.1 --out $OUT
```

<br>
So for our plink command, we did the following:

* `--vcf` - specified the location of our VCF file.
* `--double-id` - told `plink` to duplicate the id of our samples (this is because plink typically expects a family and individual id - i.e. for pedigree data - this is not necessary for us.
* `--set-missing-var-ids` - also necessary to set a variant ID for our SNPs. Human and model organisms often have annotated SNP names and so `plink` will look for these. We do not have them so instead we set ours to default to `chromosome:position` which can be achieved in `plink` by setting the option `@:#` - [see here](https://www.cog-genomics.org/plink/1.9/data#set_missing_var_ids) for more info.
* `--indep-pairwise` - finally we are actually on the command that performs our linkage pruning! The first argument, `50` denotes we have set a window of 50 Kb. The second argument, `10` is our window step size - meaning we move 10 bp each time we calculate linkage. Finally, we set an r<sup>2</sup> threshold - i.e. the threshold of linkage we are willing to tolerate. Here we prune any variables that show an r<sup>2</sup> of greater than 0.1.
* `--out` Produce the prefix for the output data.

<br>
As well as being versatile, `plink` is very fast. It will quickly produce a linkage analysis for all our data and write plenty of information to the screen. When complete, it will write out some files including two files `g1k_TOMSK_lifted_GRCh38_merged_chr1.prune.in` and `g1k_TOMSK_lifted_GRCh38_merged_chr1.prune.out`. The first of these is a list of sites which fell below our linkage threshold - i.e. those we should retain. The other file is the opposite of this. In the next step, we will produce a PCA from these linkage-pruned sites.

<br>
<br>

#### Optional Step 2: Perform a PCA
<br>


Firstly, We will make a directory called `pca`.

```{bash,  eval = FALSE}
# move out of the `ld_pruning` directory
cd ..
# make a directory
mkdir -p pca
# move into it
cd pca
```
<br>

Next we rerun plink with a few additional arguments to get it to conduct a PCA. We will run the command and then break it down as it is running.

```{bash,  eval = FALSE}
#!/bin/bash
#$ -cwd
#$ -j y
#$ -pe smp 1
#$ -l h_rt=1:0:0
#$ -l h_vmem=1G

ml plink/1.9-170906

VCF=../merge_data/g1k_TOMSK_lifted_GRCh38_merged_chr1.vcf.gz #insert your data path
PRUNE_IN=../ld_pruning/g1k_TOMSK_lifted_GRCh38_merged_chr1.prune.in
OUT=g1k_TOMSK_lifted_GRCh38_merged_chr1_pruned_maf01

plink --threads ${NSLOTS} --vcf $VCF --double-id \
--make-bed --pca --out $OUT \
--maf 0.01 \
--extract $PRUNE_IN \
--set-missing-var-ids @:#
```
<br>

This is very similar to our previous command. What did we do here?

* `--extract` - this just lets `plink` know we want to extract only these positions from our VCF - in other words, the analysis will only be conducted on these.
* `--make-bed` - this is necessary to write out some additional files for another type of population structure analysis - a model based approach with `admixture`.
* `--pca` - fairly self explanatory, this tells `plink` to calculate a principal components analysis. 

<br>

Once the command is run, we will see a series of new files. We will break these down too:

PCA output:

* `g1k_TOMSK_lifted_GRCh38_merged_chr1_pruned_maf01.eigenval` - the eigenvalues from our analysis
* `g1k_TOMSK_lifted_GRCh38_merged_chr1_pruned_maf01.eigenvec`- the eigenvectors from our analysis

plink binary output:

* `g1k_TOMSK_lifted_GRCh38_merged_chr1_pruned_maf01.bed` - the samples bed file - this is a binary file necessary for admixture analysis. It is essentially the genotypes of the pruned dataset recoded as 1s and 0s.
* `g1k_TOMSK_lifted_GRCh38_merged_chr1_pruned_maf01.bim` - a map file (i.e. information file) of the variants contained in the bed file.  
* `g1k_TOMSK_lifted_GRCh38_merged_chr1_pruned_maf01.fam` - a map file for the individuals contained in the bed file.

<br>
<br>


#### Step 2: Perform a PCA 
(using the pruned and filtered [plink 1 binary files](https://www.cog-genomics.org/plink/1.9/input#bed)) 

<br>

We could do a PCA directly from the provided plink.bed + plink.bim + plink.fam. But please look through the previous steps and understand how we got these files.

Firstly, We will make a directory called `pca`.

```{bash,  eval = FALSE}
# move out of the `ld_pruning` directory
cd ..
# make a directory
mkdir -p pca
# move into it
cd pca
```
<br>

Next we rerun plink with a few additional arguments to get it to conduct a PCA. We will run the command and then break it down as it is running.

```{bash,  eval = FALSE}
#!/bin/bash
#$ -cwd
#$ -j y
#$ -pe smp 1
#$ -l h_rt=1:0:0
#$ -l h_vmem=1G

ml plink/1.9-170906

PREFIX=../merge_data/g1k_TOMSK_lifted_GRCh38_merged_chr1_pruned_maf01 
OUT=g1k_TOMSK_lifted_GRCh38_merged_chr1_pruned_maf01

plink --threads ${NSLOTS} --bfile $PREFIX --double-id \
--pca --out $OUT
```

<br>

This is very similar to our previous command. What did we do here?

* `--pca` - fairly self explanatory, this tells `plink` to calculate a principal components analysis. 

<br>
Once the command is run, we will see a series of new files. We will break these down too.

PCA output:

* `g1k_TOMSK_lifted_GRCh38_merged_chr1_pruned_maf01.eigenval` - the eigenvalues from our analysis
* `g1k_TOMSK_lifted_GRCh38_merged_chr1_pruned_maf01.eigenvec`- the eigenvectors from our analysis

<br>
<br>

#### Step 3: Plotting the PCA output

<br>

First, we need some sort of population labels for the samples and match them with the eigenvectors we have.

```{bash, eval = FALSE}
# get the sample list in the same sequence with the .eigenvec file
awk '{print $1}' g1k_TOMSK_lifted_GRCh38_merged_chr1_pruned_maf01.eigenvec >
g1k_TOMSK_lifted_GRCh38_merged_chr1_pruned_maf01.sample
# match the sample with the population it belongs.
for i in $(cat g1k_TOMSK_lifted_GRCh38_merged_chr1_pruned_maf01.sample);do
grep -w $i ../merged_data/g1k_TOMSK_lifted_GRCh38_merged.id.pop >>
g1k_TOMSK_lifted_GRCh38_merged_chr1_pruned_maf01.sample.pop
done
#double check whether two files have the same length.
wc -l g1k_TOMSK_lifted_GRCh38_merged_chr1_pruned_maf01.sample
wc -l g1k_TOMSK_lifted_GRCh38_merged_chr1_pruned_maf01.sample.pop

```
<br>

Next we turn to R to plot the analysis we have produced!

We could use the R studio via [OnDemand](https://docs.hpc.qmul.ac.uk/ondemand/), which gives the access to the data on Apocrita. So no need to download the data to your own laptop for plotting! Or you could write an R script and submit it as a job. Whatever works for you is fine. 

<br>
<br>

##### Setting up the R environment
<br>

First load the `tidyverse` package and ensure you have `plink` output into the working directory you are operating in.

```{r, eval = FALSE}
rm(list=ls())
# load tidyverse package
library(tidyverse)
```
<br>

Then we will use a combination of `read_table` and the standard `scan` function to read in the data.

```{r, eval = FALSE}
#read in data
pca <- read_table("g1k_TOMSK_lifted_GRCh38_merged_chr1_pruned_maf01.eigenvec", col_names = FALSE)
eigenval <- scan("g1k_TOMSK_lifted_GRCh38_merged_chr1_pruned_maf01.eigenval")
```
<br>
<br>

##### Cleaning up the data
<br>

Unfortunately, we need to do a bit of legwork to get our data into reasonable shape. First we will remove a nuisance column (`plink` outputs the individual ID twice). We will also give our `pca` data.frame proper column names.

```{r, eval = FALSE}
# remove nuisance column
pca <- pca[,-1]
# set names
names(pca)[1] <- "ind"
names(pca)[2:ncol(pca)] <- paste0("PC", 1:(ncol(pca)-1))
```
<br>

Next, we will read in the populations info we generated and add a population vector.

```{r, eval = FALSE}
#read in the labels
pop <- read_table("g1k_TOMSK_lifted_GRCh38_merged_chr1_pruned_maf01.sample.pop",col_names = FALSE)
pca[,22] <- spp[,2]
# set the name
names(pca)[22] <- "populations"
```
<br>
<br>

##### Plotting the data
<br>

Now that we have done our housekeeping, we have everything in place to actually visualise the data properly. First we will plot the eigenvalues. It is quite straightforward to translate these into percentage variance explained (although note, you could just plot these raw if you wished).


```{r, eval = FALSE}
# first convert to percentage variance explained
pve <- data.frame(PC = 1:20, pve = eigenval/sum(eigenval)*100)
```
<br>


With that done, it is very simple to create a bar plot showing the percentage of variance each principal component explains.
```{r, eval = FALSE}
a <- ggplot(pve, aes(PC, pve)) + geom_bar(stat = "identity",fill = "gray",colour = "black")
a + ylab("Percentage variance explained") + theme_classic()
```
<br>


Cumulatively, they explain 100% of the variance but PC1, PC2 and possible PC3 together explain about 30% of the variance. We could calculate this with the `cumsum` function, like so:
```{r, eval = FALSE}
# calculate the cumulative sum of the percentage variance explained
cumsum(pve$pve)
```
<br>


Next we move on to actually plotting our PCA. Given the work we did earlier to get our data into shape, this doesn’t take much effort at all.
```{r, eval = FALSE}
b <- ggplot(pca, aes(PC1, PC2, col = populations)) + geom_point(size = 0.8,alpha=1)
b + xlab(paste0("PC1 (", signif(pve$pve[1], 3), "%)")) + 
  ylab(paste0("PC2 (", signif(pve$pve[2], 3), "%)")) +
  theme_bw()
```

<br>

--------------------------------------------------

<br>

## Compute per site *F*<sub>ST</sub>
<br>

In the previous sessions, we investigated population structure. Now we will return to our filtered, high-quality variant set stored in our VCF and learn how to calculate population differentiation.

<br>

#### Optional Step 1: Calculating the Mean genome-wide *F*<sub>ST</sub>

<br>

As a first step, we will calculate a mean genome-wide *F*<sub>ST</sub> value for our variant set. This is simple to do and there are many tools that can do it. We will use `plink`.

Let's create a directory called `fst`.

```{bash, eval = FALSE}
# make a directory
mkdir -p fst
```
<br>

As we learned earlier, VCFs can store a lot of information, but one thing they do not contain is population information. They can store it indirectly - i.e. in the sample names - but not explicitly in the metadata. This means that in order to calculate pairwise *F*<sub>ST</sub>, we need to first create files containing each pair of populations.

We will focus our analysis between the Tomsk population and other populations
```{bash, eval = FALSE}
cd merged_data
# generate populations files in plink format for each pair of populations
for POP in AFR AMR EAS EUR SAS;do
grep Sample g1k_TOMSK_lifted_GRCh38_merged.id.pop > TOMSK_$POP.id.pop
grep $POP g1k_TOMSK_lifted_GRCh38_merged.id.pop >> TOMSK_$POP.id.pop
awk '{print $1,$1,$2}' TOMSK_$POP.id.pop > TOMSK_$POP.id.pop.plink
done
#move into our fst directory
cd ../fst
```

<br>

We now have two population files that we can use for our  *F*<sub>ST</sub> analyses. We are ready to use `plink`.
```{bash, eval = FALSE}
#!/bin/bash
#$ -cwd
#$ -j y
#$ -pe smp 1
#$ -l h_rt=1:0:0
#$ -l h_vmem=1G

ml plink/1.9-170906

for POP in AFR AMR EAS EUR SAS;do

POPFILE=../merge_data/TOMSK_${POP}.id.pop.plink
VCF=../merge_data/g1k_TOMSK_lifted_GRCh38_merged_chr1.vcf.gz
OUT=TOMSK_${POP}_chr1

plink --vcf $VCF --double-id \
--set-missing-var-ids @:# \
--within $POPFILE \
--fst \
--out $OUT

done
```

<br>
<br>

#### Step 2: Check the Mean genome-wide *F*<sub>ST</sub>
<br>

`plink` will run for a few moments and generate some files. There are some `.log` files. You can see from this it calculates the mean *F*<sub>ST</sub> across all the SNPs we provided on chromsome 1 for each populations pair.


First, let's check the mean *F*<sub>ST</sub>.
```{bash, eval = FALSE}
grep Fst *log
```

<br>

So what did we do here? Well we calculated Weir & Cockerham's *F*<sub>ST</sub> for each SNP and to do this, all we needed to do was provide a sample file containing two populations using the [`--within` flag](https://www.cog-genomics.org/plink/1.9/input#within), and the [`--fst` flag](https://www.cog-genomics.org/plink/1.9/basic_stats#fst).

Now that we have created our per-snp *F*<sub>ST</sub> estimates, it's a good time to have a look at the output using tools like `head` or `less`. Of course, you can only gain so much information by looking at a bunch of values like this - it makes much more sense to plot them. So for that we turn to `R`.

<br>
<br>

#### Step 3: Visualising per SNP *F*<sub>ST</sub>
<br>

We will load the `tidyverse` package.

```{r, eval = FALSE}
rm(list=ls())
# load tidyverse package
library(tidyverse)
```
<br>

Next, we will read in the `.fst` file of Tomsk and African populations.
```{r, eval = FALSE}
# read in the file
fst <- read_tsv("TOMSK_AFR_chr1.fst")
```
<br>

With that, we can plot our data using `ggplot2`:
```{r, eval = FALSE}
ggplot(fst, aes(POS, FST)) + geom_point()
```
<br>

This plot is extremely simple but you can see there is a lot of noise - that's because there are a lot of SNPs... and this is only for a single chromosome! It is often easier to perform such analyses on sliding windows across the genome, because then it is easier to see overall trends and patterns in the data. There is less of an issue with the stochasticity of *F*<sub>ST</sub> at neighbouring polymorphic positions.

However before we move on, we will perform a simple outlier analysis.

<br>
<br>

#### Step 4: Performing a simple outlier analysis

<br>

There are many, many methods for performing outlier tests. Some use coalescent simulations, others use Bayesian inference to estimate the posterior probability that a SNP is an outlier and therefore putatively under selection.

However, the simplest way to detect outliers is to use the empirical distribution of *F*<sub>ST</sub> to look for SNPS that exceed an arbitrary threshold of differentiation. This method has been used quite a lot in the literature but it is not without caveats (and more on those later). Typically the threshold is set at either the 95th or 99th percentile of the empirical data.

Once again, we can turn to `R` to do this quite easily. First, let's identify the thresholds from the distribution.

```{r, eval = FALSE}
# identify the 95% and 99% percentile
quantiles(fst$FST, c(0.975, 0.995), na.rm = T)
```
<br>

Hang on a moment, those values aren't 95 and 99! That's because we are performing a one-tailed test here, so we are not interested in the lower tail of the *F*<sub>ST</sub> distribution.

Now we know how to identify our threshold, let's identify which SNPs are outliers above the 95% threshold. This will also make it possible for us to plot them more easily later. To do this, we will use the `ifelse` function.

```{r, eval = FALSE}
# identify the 95% percentile
my_threshold <- quantile(fst$FST, 0.975, na.rm = T)
# make an outlier column in the data.frame
fst <- fst %>% mutate(outlier = ifelse(FST > my_threshold, "outlier", "background"))
```
<br>

What did we do here? `ifelse` takes a logical argument first (i.e. is fst greater than the threshold we set). If this is the case (i.e. it is `TRUE`), it will print "outlier" but if not, it just prints "background".

Now we can see how many outlier SNPs we have versus the background:

```{r, eval = FALSE}
fst %>% group_by(outlier) %>% tally()
```
<br>

Finally, let’s recreate our plot and this time colour the SNPs by their outlier status.
```{r, eval = FALSE}
ggplot(fst, aes(POS, FST, colour = outlier)) + geom_point(size=0.8,alpha=0.5)+
  scale_color_manual(values=c("steelblue","darkred"))+
    labs(title = "TOMSK_AFR",x ="chr1 position (bp)", y = expression(paste(italic(F)[ST])))+
    theme(legend.position = "none") +
    scale_x_continuous(labels = scales::comma)
```
<br>

Now we can actually see the position of our outlier SNPs along the chromsome. Downstream, we might want to see what genes these occur close to. But for now, think a bit about how much we might trust this test…

<br>

##### *Bonus Step 4: Performing simple outlier analyses using a loop*


```{r, eval = FALSE}
# read in the file
pops <- c("AFR", "AMR", "EAS", "EUR", "SAS")

pdf(file="TOMSK_fst_chr1.pdf",width=10,height=2.5)

for (pop in pops){
  fst <- read_tsv(paste0("TOMSK_",pop,"_chr1.fst"))
  # identify the 95% percentile
  my_threshold <- quantile(fst$FST, 0.975, na.rm = T)
  fst <- fst %>% mutate(outlier = ifelse(FST > my_threshold, "outlier", "background"))
  fst %>% group_by(outlier) %>% tally()
  ggplot(fst, aes(POS, FST, colour = outlier)) + geom_point(size=0.8,alpha=0.5)+
    theme_bw() + scale_color_manual(values=c("steelblue","darkred"))+
    labs(title = paste0("TOMSK_",pop), x = "chr1 position (bp)", y = expression(paste(italic(F)[ST])))+
    theme(legend.position = "none") +
    scale_x_continuous(labels = scales::comma)
}

dev.off()

```
<br>



--------------------------------------------------
<br>

## Estimate the site frequency spectrum

<br>

In order to perform demographic analyses with programs such as `fastsimcoal2` or `dadi`,
you need to generate or estimate a **site-frequency spectrum** (SFS). There are multiple different ways to do this but one of the simplest is using [easysfs](https://github.com/isaacovercast/easySFS), a really nice python based utility that is built on top of the `dadi` libraries.

However, we will continue to use plink today to generate simpe SFS.

<br>

Let's create a directory called `sfs`.

```{bash, eval = FALSE}
# make a directory
mkdir -p sfs
cd sfs
```
<br>

#### Optional Step 1: calculate the allele frequency 
<br>

We need to calculate the allele frequency for each population separately. So instead of running the same process for many times, we could use an [array job](https://docs.hpc.qmul.ac.uk/using/arrays/). 

First of all, we need a file which I just called `file_list`. It looks like this, with the first column stating the population name, the second column stating the path to the vcf file.

```{bash, eval = FALSE}
cat file_list
```

```
TOMSK ../data/TOMSK_lifted_GRCh38_chr1.vcf.gz
AFR ../1000G_phased/AFR/AFR_chr1_biallelic_snps.vcf.gz
AMR ../1000G_phased/AMR/AMR_chr1_biallelic_snps.vcf.gz
EAS ../1000G_phased/EAS/EAS_chr1_biallelic_snps.vcf.gz
EUR ../1000G_phased/EUR/EUR_chr1_biallelic_snps.vcf.gz
SAS ../1000G_phased/SAS/SAS_chr1_biallelic_snps.vcf.gz
```
<br>

Next, submit the array job!
```{bash, eval = FALSE}
#!/bin/bash
#$ -cwd
#$ -j y
#$ -pe smp 1
#$ -l h_rt=1:0:0
#$ -l h_vmem=1G
#$ -t 1-6


POP=$(sed -n "${SGE_TASK_ID}p" file_list | awk '{print $1}')
VCF=$(sed -n "${SGE_TASK_ID}p" file_list | awk '{print $2}')

ml plink/1.9-170906
plink --vcf $VCF --double-id \
--set-missing-var-ids @:# \
--freq counts \
--out ${POP}_chr1_biallelic_snps
```
<br>

One thing to notice is that the SFS cannot be computed with sites that contain missing data. By projecting down the number of individuals per population, more sites can be recovered. Alternatively, if the dataset has a lot of missing data and low coverage, you may want to generate the SFS with [ANGSD](http://www.popgen.dk/angsd/index.php/ANGSD) which takes genotype uncertainties into account.

<br>
<br>

#### Step 2: plot the SFS 
<br>

Next we turn to R to plot the analysis we have produced, again!

```{r, eval = FALSE}
rm(list=ls())
library(tidyverse)

pdf(file=paste0(pop,"_chr1_biallelic_snps.pdf"),width=4,height=4)
pops <- c("TOMSK","AFR", "AMR", "EAS", "EUR", "SAS")
for (pop in pops){
  var_counts <- read.table(paste0(pop, "_chr1_biallelic_snps.frq.counts"), header = TRUE)
  var_counts2 <- filter(var_counts,var_counts$C1!=0&var_counts$C2!=0)
  a <- ggplot(var_counts2, aes(C1)) + geom_histogram(fill = "gray", colour = "black",binwidth = 1)
  a+theme_classic()+labs(x="",y="", title = pop) 
}
dev.off()
```

